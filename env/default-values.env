# Note: You only need to set the variables you normally would with '-e' flags.
# You do not need to set them all if they will go unused.

# Enable Inference Providers
## Set any providers you want enabled to 'true'
## E.g. ENABLE_VLLM=true
## Leave all disabled providers EMPTY
## E.g. ENABLE_OPENAI=
ENABLE_VLLM=
ENABLE_VERTEX_AI=
ENABLE_OPENAI=
ENABLE_OLLAMA=

# vLLM Inference Settings
VLLM_URL=
VLLM_API_KEY=
# vLLM Optional Variables
VLLM_MAX_TOKENS=
VLLM_TLS_VERIFY=

# OpenAI Inference Settings
OPENAI_API_KEY=

# Vertex AI Inference Settings
VERTEX_AI_PROJECT=
VERTEX_AI_LOCATION=
GOOGLE_APPLICATION_CREDENTIALS=

# Ollama Inference Settings
OLLAMA_URL=

# Question Validation Safety Shield Settings
## Ensure VALIDATION_PROVIDER is one of your enabled Inference Providers
## E.g. VALIDATION_PROVIDER=vllm if ENABLE_VLLM=true
VALIDATION_PROVIDER=
VALIDATION_MODEL_NAME=

# Llama Guard Settings
## Defaults to llama-guard3:8b if not set
SAFETY_MODEL=
## Defaults to http://host.docker.internal:11434/v1 if not set
SAFETY_URL=
## Only required for non-local environments with a api key
SAFETY_API_KEY=

# Other
LLAMA_STACK_LOGGING=